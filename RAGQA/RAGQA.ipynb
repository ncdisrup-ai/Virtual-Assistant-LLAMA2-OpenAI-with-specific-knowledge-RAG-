{"cells":[{"cell_type":"markdown","metadata":{"id":"JPdQvYmlWmNc"},"source":["# Open-Source RAG with LLaMa 13B (4 bits for less GPU memory), Faiss, HuggingFace and Langchain or with OpenAI\n","\n","In this Poc we'll create a RAG Open-Source solution with **Llama-13b-chat** with HuggingFace embedings, Faiss (Vector DB), all orchestrated by LangChain. Or we could parametrize with OpenAI.\n","\n","In terms of struture of the solution, we have the main UI in file\n"," `RAG_QAw_Parametrization.ipynb` that import all the parametrization (which model, temperature, chain...) from `parametrization.ipynb`  and the core RAG functions from `RAGQA.ipynb`. `RAGQA.ipynb` import also `Parametrization.ipynb`.   \n","\n","**Retrieval Augmented Generation (RAG)** is an advanced Natural Language Processing (NLP) technique that combines both retrieval and generation elements to enhance AI language models' capabilities.\n","\n","You must first request access to Llama 2 models via [this form](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) (access is typically granted within a few hours).\n","---\n","\n","ğŸš¨ I suggest  runing in Google Colab  by going to **Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**. This should be included within the free tier of Colab.\n","\n","---\n","\n","We start by doing a `pip install` of all required libraries."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42959,"status":"ok","timestamp":1696241059673,"user":{"displayName":"Nuno cepeda","userId":"00894198540040655576"},"user_tz":-60},"id":"K_fRq0BSGMBk","outputId":"d6f8221b-617e-4026-a7fb-a96bb59e963f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m276.3/276.3 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m298.4/298.4 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for Docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -qU \\\n","  transformers==4.31.0 \\\n","  sentence-transformers==2.2.2 \\\n","  datasets==2.14.0 \\\n","  accelerate==0.21.0 \\\n","  einops==0.6.1 \\\n","  langchain==0.0.240 \\\n","  xformers==0.0.20 \\\n","  bitsandbytes==0.41.0 \\\n","  pypdf \\\n","  faiss-cpu \\\n","  Docx2txt \\\n","  gradio \\\n","  openai \\\n","  import-ipynb"]},{"cell_type":"markdown","metadata":{"id":"7dosdUoggxpT"},"source":["# Import modules"]},{"cell_type":"markdown","source":["##For import other ipynb (Collab) notebooks\n","\n","*Texto em itÃ¡lico*\n"],"metadata":{"id":"VQYDbSN0jSyx"}},{"cell_type":"code","source":["#drive.mount(\"/content/drive\", force_remount=True)\n","#/content/drive/MyDrive/Colab Notebooks"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XokLGiyplJgK","executionInfo":{"status":"ok","timestamp":1696119268272,"user_tz":-60,"elapsed":2750,"user":{"displayName":"Nuno cepeda","userId":"00894198540040655576"}},"outputId":"efe06530-4b21-4a1d-a869-2d331e7a29e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#authorize Colab to access your Google Drive account, only necessary when running this module, not when importing\n","#from google.colab import drive\n","#drive.mount('/content/drive')\n","\n","#Google Drive will be mounted at /content/drive\n","\n","#directory where your IPYNB file is located\n","#%cd /content/drive/MyDrive/Colab Notebooks"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L5wXKa2HjqTB","executionInfo":{"status":"ok","timestamp":1696241099911,"user_tz":-60,"elapsed":24264,"user":{"displayName":"Nuno cepeda","userId":"00894198540040655576"}},"outputId":"51887676-4585-4d31-d3b5-6ff3d7e97bf1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYUgkkArgsgA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696241121074,"user_tz":-60,"elapsed":13626,"user":{"displayName":"Nuno cepeda","userId":"00894198540040655576"}},"outputId":"6f6d83b6-7af0-4c23-a1c3-5a8baa88b51c"},"outputs":[{"output_type":"stream","name":"stdout","text":["importing Jupyter notebook from Parametrization.ipynb\n"]}],"source":["from langchain.text_splitter import CharacterTextSplitter\n","from langchain.document_loaders import TextLoader\n","from langchain.document_loaders import PyPDFLoader\n","from langchain.document_loaders import Docx2txtLoader\n","from langchain.chains.question_answering import load_qa_chain\n","from langchain.chains import (\n","     LLMChain, ConversationalRetrievalChain\n",")\n","\n","from langchain import PromptTemplate\n","from langchain.memory import ConversationBufferMemory\n","from langchain.memory import ConversationBufferWindowMemory\n","from langchain.vectorstores import FAISS\n","\n","import gradio as gr\n","\n","#%run Parametrization.ipynb\n","import import_ipynb\n","\n","import Parametrization as pr"]},{"cell_type":"markdown","metadata":{"id":"NvJyZMbhiEbo"},"source":["# Global Variables"]},{"cell_type":"markdown","source":[],"metadata":{"id":"bTVy6pLmpPgl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GlVafixdiJaf"},"outputs":[],"source":["#Global Variables with access functions\n","vectorstore = None\n","KnowledgeUploaded = False"]},{"cell_type":"markdown","metadata":{"id":"fK7OXFdulxo6"},"source":["##Access functions to global variables for solution parametrization\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQf0ICZXmGPq"},"outputs":[],"source":["#Access functions to global variables\n","def vectorstore_read ():\n","    return vectorstore\n","\n","def vectorstore_write (docs, embeddings):\n","    global vectorstore\n","\n","    vectorstore = FAISS.from_documents(docs, embeddings)\n","    return vectorstore\n","\n","def vectorstore_docs (query):\n","\n","    vectorstore = vectorstore_read()\n","    docs = vectorstore.similarity_search(query)\n","    return docs\n","\n","\n","def KnowledgeUploaded_read ():\n","    return KnowledgeUploaded\n","\n","def KnowledgeUploaded_write (new_value):\n","    global KnowledgeUploaded\n","\n","    KnowledgeUploaded = new_value\n","    return KnowledgeUploaded"]},{"cell_type":"markdown","metadata":{"id":"VHQwEeW9Zps2"},"source":["## Create the knowledge base\n","\n","Create a knowledge base with documents (type pdf; docx and txt), same dir, for the specific knowledge of the virtual assistant (Question/Anwswer and Chat)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ikzdi_uMI7B-"},"outputs":[],"source":["#Create a knowledge base with documents (type pdf; docx and txt)\n","def process_knowledge (pathFiles):\n","    documents = []\n","    for file in pathFiles:\n","        file2Treat = file.name  # obtain filename path (from Gradio)\n","        if file2Treat.endswith(\".pdf\"):\n","            loader = PyPDFLoader(file2Treat)\n","            documents.extend(loader.load())\n","        elif file2Treat.endswith('.docx') or file2Treat.endswith('.doc'):\n","            loader = Docx2txtLoader(file2Treat)\n","            documents.extend(loader.load())\n","        elif file2Treat.endswith('.txt'):\n","            loader = TextLoader(file2Treat)\n","            documents.extend(loader.load())\n","\n","    docs_not_splitted = documents\n","\n","    # Split the documents into smaller chunks\n","    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200) #try\n","    docs = text_splitter.split_documents(docs_not_splitted)\n","\n","    # embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n","    embeddings = pr.embeddings_read ()\n","\n","    #FAISS to work with ConversationalRetrievalChain\n","    #vectorstore = FAISS.from_documents(docs, embeddings)\n","    #vectorstore = Chroma.from_documents(docs, embeddings) # Chroma DB, problems in installing in env....\n","    vectorstore = vectorstore_write (docs, embeddings)\n","\n","    KnowledgeUploaded = KnowledgeUploaded_write (True)\n","    msg = \"docs uploaded in Vector DB\"\n","    return msg"]},{"cell_type":"markdown","metadata":{"id":"JzX9LqWSX9ot"},"source":["##Prompt Bullets\n","\n","it returns a Prompt for a customer support chatbot that answer questions using\n","information extracted from our knowledge base. Having the answer in bullets or no"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0iPv1GDGxgT"},"outputs":[],"source":["def promt_bullets (bullets):\n","\n","    if pr.bulletprompt2work_read():\n","        template = \"\"\"You are a exceptional customer support virtual assistant having a conversation with a human.\n","\n","        Given the following context information and a question, create a final answer in bullets.\n","\n","        {context}\n","\n","        Human: {human_input}\n","        Virtual Assistant:\"\"\"\n","        # bullet = \"in bullets\"\n","    else:\n","        template = \"\"\"You are a exceptional customer support virtual assistant having a conversation with a human.\n","\n","        Given the following context information and a question, create a final answer.\n","\n","        {context}\n","\n","        Human: {human_input}\n","        Virtual Assistant:\"\"\"\n","        # bullet = \"\"\n","        # remove {chat_history}\n","    return template"]},{"cell_type":"markdown","metadata":{"id":"bNysQFtPoaj7"},"source":["##Question Answer (Q/A)\n","\n","Question Answer (Q/A) from the Knowledge base, has the query (question) as an input and will return the answer. For RAG technique"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qAYXi8ayKusU"},"outputs":[],"source":["def question_answer(query):\n","    # define the prompt with bullets or no\n","    template = promt_bullets(pr.bulletprompt2work_read())\n","    # validate if vectorstore exists....\n","    vectorst = vectorstore_read()\n","    if vectorst:\n","        docs = vectorstore_docs(query)\n","        #docs = vectorstore.similarity_search(query)  # db\n","        prompt = PromptTemplate(\n","            input_variables=[\"human_input\", \"context\"], template=template\n","        )\n","        # memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"human_input\")\n","        if pr.qachain2work_read() == pr.LOADQA:\n","            chain = load_qa_chain(\n","                pr.llm_read(), chain_type=\"stuff\", prompt=prompt, #temperature inside llm\n","                verbose=pr.verbose2work_read()\n","            )  # no memory\n","            result = chain({\"input_documents\": docs, \"human_input\": query}, return_only_outputs=True)\n","            output_text = result[\"output_text\"]\n","            #print(f\"result[output_text] {output_text}\")\n","            return result[\"output_text\"]\n","        # elif chain retrieve\n","        elif pr.qachain2work_read() == pr.RETRIEVALQA:\n","            from langchain.chains import RetrievalQA\n","\n","            rag_pipeline = RetrievalQA.from_chain_type(\n","                llm=pr.llm_read(),\n","                chain_type='stuff',\n","                retriever=vectorst.as_retriever(),\n","                return_source_documents=True,\n","                verbose=pr.verbose2work_read()#,\n","                #chain_type_kwargs={\"prompt\": prompt}\n","            )\n","\n","            # retrievalQA chain only uses query as an input, in order to have a custom made prompt\n","            # we need to go lower for instance with load_qa_chain, or  changing parameters definition in retrieval...in\n","            result = rag_pipeline(query)\n","            # result = rag_pipeline ({\"input_documents\": docs, \"human_input\": query}, return_only_outputs=True)\n","            # ValueError: Missing some input keys:  {'query'}\n","            #output_text = result[\"output_text\"]\n","            #return output_text\n","            return result[\"result\"]\n","    # elseif  isnull vectorstore:\n","    print(\"Upload knowledge please\")  # if arrives here vectorstore is Null:\n","    return \"Upload knowledge please\""]},{"cell_type":"markdown","metadata":{"id":"0N3W3cj3Re1K"},"source":["#User Interface (UI) for knowledge, QA and Chat\n","\n","Creating the specific knowledge base, QA and  Chat function with memory.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-8RxQYwHRg0N"},"outputs":[],"source":["gr.close_all()\n","\n","CSS =\"\"\"\n",".contain { display: flex; flex-direction: column; }\n",".gradio-container { height: 100vh !important; }\n","#component-0 { height: 100%; }\n","#chatbot { flex-grow: 1; overflow: auto;}\n","\"\"\"\n","\n","with gr.Blocks(css=CSS) as app_vassistant_qa:\n","    gr.Markdown(\"# Virtual Assistant, with specific knowledge,  for Question Answering \")\n","\n","    with gr.Tab(label = \"Upload specific Knowledge Base\"):\n","        DirUploaded = gr.File(label=\"Upload files of Knowledge\", type=\"file\", file_count=\"multiple\") # file_types = [\"text\"];  file_count = \"directory\"\n","        btnProcessKnowledge = gr.Button(\"Upload Knowledge Base\")  # Submit button side by side!\n","        msgUpload = gr.Textbox(label=\"Knowledge Base Uploaded?\")\n","\n","    with gr.Tab(label = \"Question Answering\"):\n","        with gr.Row():  # only visible when btn.click\n","            with gr.Column(scale=2):\n","                question = gr.Textbox(label=\"Your Question\")  # Question\n","                btnProcessQuestion = gr.Button(\"Submit Question\")\n","            with gr.Column(scale=4, min_width=50):\n","                output = gr.Textbox(label=\"Answer\", lines=12)\n","\n","    with gr.Tab(label=\"Virtual Assistant with specific knowledge, in chat format\"):\n","        chatbot = gr.Chatbot(elem_id=\"chatbot\")\n","        msg = gr.Textbox(label=\"Human message\")\n","        msgNeedSpecificKnowledge = gr.Textbox(label=\"Need to Upload Specific Knowledge?\")\n","        clear = gr.ClearButton([msg, chatbot, msgNeedSpecificKnowledge])\n","\n","        # Q/A in chat format with memory\n","        def respond(query, chat_history_G):\n","            from langchain.schema import AIMessage, HumanMessage\n","\n","            if KnowledgeUploaded_read () == False: # \"need to upload knowledge\"\n","                msgNeedSpecificKnowledge = \"Need to Upload Specific Knowledge Base\"\n","\n","            elif KnowledgeUploaded_read (): #There is knowledge, we can go on with chat\n","                msgNeedSpecificKnowledge = \"\"\n","                # In each chat iteration we need to re-create the chain variables\n","                # may change (parametrizatiom, knowledge base)because we have parameters so each iteration we will need to define the chat\n","\n","                # define the prompt with bullets or no\n","                template = promt_bullets(pr.bulletprompt2work_read())\n","                vectorst = vectorstore_read()\n","\n","                if pr.qachain2work_read() == pr.RETRIEVALQA:\n","                    qachain = ConversationalRetrievalChain.from_llm(\n","                        pr.llm_read(),  # has got temperature\n","                        vectorst.as_retriever(),\n","                        # retriever = retriever,\n","                        # db.as_retriever(),\n","                        # memory=memory, without  memoria.....we control explicitly the memory\n","                    )\n","                elif pr.qachain2work_read() == pr.LOADQA: #Retrieval =... think about other?\n","\n","                    qachain = ConversationalRetrievalChain.from_llm(\n","                        llm = pr.llm_read(),  # has got temperature\n","                        retriever = vectorst.as_retriever(),\n","                        verbose=pr.verbose2work_read()\n","                        #chain_type_kwargs={\"prompt\": prompt} #?\n","                        # retriever = retriever,\n","                        # db.as_retriever(),\n","                        # memory=memory, without  memoria.....we control explicitly the memory\n","                    )\n","\n","                memory = ConversationBufferWindowMemory(memory_key=\"chat_history\",return_messages=True) #AT so criar uma vez?\n","                history_langchain_format = []\n","                for human, ai in chat_history_G:\n","                    history_langchain_format.append(HumanMessage(content=human))\n","                    history_langchain_format.append(AIMessage(content=ai))\n","\n","                result = qachain({\"question\": query,\n","                                \"chat_history\": history_langchain_format}) # input_documents no need because qachain deals with it (has vectorstore)\n","\n","                chat_history_G.append((query, result[\"answer\"]))\n","\n","            return \"\", chat_history_G, msgNeedSpecificKnowledge\n","\n","        msg.submit(respond, [msg, chatbot], [msg, chatbot, msgNeedSpecificKnowledge])\n","\n","    btnProcessKnowledge.click(fn=process_knowledge, inputs=DirUploaded, outputs=msgUpload)  # output: gradio components nop\n","    btnProcessQuestion.click(fn=question_answer, inputs=question, outputs=output)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1yGwdReT2mxUetg3-mCX_Wxq7hMvi85Jp","timestamp":1696099908738},{"file_id":"1qgok4GptGdyRBcHTSedbHKYbujLQI7gL","timestamp":1695889726462},{"file_id":"https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb","timestamp":1695473134898}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}